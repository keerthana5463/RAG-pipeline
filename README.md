# RAG-pipeline
I developed a Retrieval-Augmented Generation (RAG) pipeline that integrates a retrieval system with a generative language model to deliver context-aware and accurate responses. 
This project demonstrates a **basic Retrieval Augmented Generation (RAG)** pipeline using **LangChain** and **ChromaDB**.  
The system allows users to query custom documents and receive accurate, context-aware answers generated by an LLM.

This is a beginner-level project aimed at understanding how RAG works step by step.

---
## ğŸš€ Features
- Load custom documents
- Split documents into chunks
- Convert text into vector embeddings
- Store vectors in ChromaDB
- Retrieve relevant documents for a query
- Generate answers using an LLM with retrieved context
## ğŸ› ï¸ Technologies Used
- Python
- LangChain
- ChromaDB (Vector Database)
- OpenAI / LLM APIs
- Tiktoken
- Jupyter Notebook
## ğŸ§© Project Workflow
1. Load documents
2. Split text into smaller chunks
3. Generate embeddings for each chunk
4. Store embeddings in ChromaDB
5. Retrieve relevant chunks based on user query
6. Generate a final response using LLM
